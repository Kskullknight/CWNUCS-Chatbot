from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import json
import time
import sys
import re
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading

# ì „ì—­ ë½ (JSON íŒŒì¼ ì“°ê¸° ë™ê¸°í™”ìš©)
file_lock = Lock()
progress_lock = Lock()
progress_data = {'completed': 0, 'total': 0}

def html_to_markdown(html):
    soup = BeautifulSoup(html, 'html.parser')
    for table in soup.find_all('table'):
        table_md = []
        # í—¤ë”
        headers = [th.get_text(strip=True) for th in table.find_all('th')]
        if headers:
            table_md.append('| ' + ' | '.join(headers) + ' |')
            table_md.append('|' + '---|' * len(headers))
        # ë°ì´í„° í–‰
        for tr in table.find_all('tr'):
            cols = [td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]
            if cols:
                table_md.append('| ' + ' | '.join(cols) + ' |')
        table.replace_with('\n'.join(table_md))

    # ë§í¬ ì²˜ë¦¬
    for a in soup.find_all('a'):
        link_text = a.get_text(strip=True)
        link_url = a.get('href', '')
        if link_url:
            a.replace_with(f'[{link_text}]({link_url})')
        else:
            a.replace_with(link_text)

    # ë³¼ë“œ, ì´íƒ¤ë¦­ ë“± ë‹¨ìˆœ ë³€í™˜
    for b in soup.find_all(['b', 'strong']):
        b.string = f"**{b.get_text(strip=True)}**"
    for i in soup.find_all(['i', 'em']):
        i.string = f"*{i.get_text(strip=True)}*"

    # ul/ol/li ëª©ë¡ ì²˜ë¦¬
    for ul in soup.find_all('ul'):
        items = []
        for li in ul.find_all('li'):
            items.append(f"- {li.get_text(strip=True)}")
        ul.replace_with('\n'.join(items))
    for ol in soup.find_all('ol'):
        items = []
        for idx, li in enumerate(ol.find_all('li'), 1):
            items.append(f"{idx}. {li.get_text(strip=True)}")
        ol.replace_with('\n'.join(items))

    # ì¤„ë°”ê¿ˆ <br> ì²˜ë¦¬
    for br in soup.find_all('br'):
        br.replace_with('\n')

    # ê° p íƒœê·¸ëŠ” í•œ ì¤„ë¡œ ë³€í™˜
    lines = []
    for child in soup.find_all(['p', 'table']):
        text = child.get_text(separator=' ', strip=True)
        # í…Œì´ë¸”ì€ ì´ë¯¸ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ë˜ì–´ textì— ìˆìŒ
        if child.name == 'table':
            lines.append(text)
        else:
            if text:
                lines.append(text)
        # p íƒœê·¸ ì²˜ë¦¬ í›„ ì œê±°(ì¤‘ë³µ ë°©ì§€)
        child.decompose()

    # p/table ì´ì™¸ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸(ì£¼ì˜: í…Œì´ë¸”, pë¥¼ ëª¨ë‘ ì²˜ë¦¬í•œ ë’¤ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
    body_text = soup.get_text(separator='\n', strip=True)
    body_text = re.sub(r'\n+', '\n', body_text)
    if body_text:
        lines.append(body_text)

    # ë¶ˆí•„ìš”í•œ ì—°ì† ê°œí–‰ ì œê±°
    markdown = '\n'.join([line.strip() for line in lines if line.strip()])
    return markdown

def setup_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì • (ìŠ¤ë ˆë“œë³„ë¡œ ë…ë¦½ì ì¸ ë“œë¼ì´ë²„ ìƒì„±)"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ì¶”ê°€ ì˜µì…˜
    chrome_options.add_argument('--disable-features=VizDisplayCompositor')
    
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

def get_notice_list(driver, page_num=1):
    """íŠ¹ì • í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    list_url = f'https://www.changwon.ac.kr/ce/na/ntt/selectNttList.do?mi=6627&bbsId=2187&currPage={page_num}'
    driver.get(list_url)
    time.sleep(2)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    notices = []
    
    tbody = soup.find('tbody')
    if tbody:
        rows = tbody.find_all('tr')
        for row in rows:
            tds = row.find_all('td')
            if len(tds) >= 6:
                num_text = tds[0].text.strip()
                # ì¼ë°˜ ê²Œì‹œê¸€ë§Œ (ê³µì§€ ì œì™¸)
                if num_text.isdigit():
                    link = tds[1].find('a')
                    if link:
                        ntt_sn = link.get('data-id')
                        if ntt_sn:
                            notice_info = {
                                'number': num_text,
                                'ntt_sn': ntt_sn,
                                'title': link.text.strip(),
                                'author': tds[2].text.strip(),
                                'date': tds[3].text.strip(),
                                'views': tds[4].text.strip()
                            }
                            notices.append(notice_info)
    
    return notices

def scrape_notice_detail(driver, ntt_sn):
    """ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
    detail_url = f'https://www.changwon.ac.kr/ce/na/ntt/selectNttInfo.do?mi=6627&nttSn={ntt_sn}'
    driver.get(detail_url)
    time.sleep(1)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    notice_data = {
        'url': detail_url,
        'ntt_sn': ntt_sn
    }
    
    # BD_table í´ë˜ìŠ¤ ë‚´ë¶€ì˜ í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
    bd_table = soup.find('div', class_='BD_table')
    if bd_table:
        table = bd_table.find('table')
        if table:
            rows = table.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                
                if th and td:
                    label = th.text.strip()
                    value = td.text.strip()
                    
                    if th.get('class') and 'title' in th.get('class'):
                        notice_data['title'] = value
                    elif 'ì‘ì„±ì' in label:
                        notice_data['author'] = value
                    elif 'ì‘ì„±ì¼' in label or 'ë“±ë¡ì¼' in label:
                        notice_data['date'] = value
                    elif 'ì¡°íšŒ' in label:
                        notice_data['views'] = value
                    elif 'ì²¨ë¶€íŒŒì¼' in label:
                        attachments = []
                        file_links = td.find_all('a')
                        for link in file_links:
                            file_name = link.text.strip()
                            if file_name:
                                attachments.append({
                                    'name': file_name,
                                    'onclick': link.get('onclick', '')
                                })
                        notice_data['attachments'] = attachments
    
    # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
    content_div = soup.find('div', class_='ntt_cn')
    if content_div:
        outer_html = str(content_div).replace('\n', '').replace('\r', '')
        markdown_content = html_to_markdown(outer_html)
        notice_data['content'] = markdown_content
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = []
        img_tags = content_div.find_all('img')
        for img in img_tags:
            img_data = {
                'src': img.get('src', ''),
                'alt': img.get('alt', ''),
                'width': img.get('width', ''),
                'height': img.get('height', '')
            }
            if img_data['src']:
                if img_data['src'].startswith('/'):
                    img_data['src'] = 'https://www.changwon.ac.kr' + img_data['src']
                images.append(img_data)
        
        if images:
            notice_data['images'] = images
        
        # ë§í¬ ì¶”ì¶œ
        links = []
        link_tags = content_div.find_all('a')
        for link in link_tags:
            link_data = {
                'href': link.get('href', ''),
                'text': link.text.strip()
            }
            if link_data['href']:
                links.append(link_data)
        
        if links:
            notice_data['links'] = links
    
    # ì½˜í…ì¸  íƒ€ì… ê²°ì •
    has_content = bool(notice_data.get('content'))
    has_images = bool(notice_data.get('images'))
    
    if has_images and not has_content:
        content_type = 'image_only'
    elif has_content and has_images:
        content_type = 'text_and_image'
    elif has_content:
        content_type = 'text_only'
    else:
        content_type = 'empty'
    
    notice_data['content_type'] = content_type
    
    return notice_data

def scrape_page(page_num, pages_to_scrape):
    """íŠ¹ì • í˜ì´ì§€ì˜ ëª¨ë“  ê²Œì‹œê¸€ í¬ë¡¤ë§ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
    thread_id = threading.current_thread().name
    driver = setup_driver()
    page_notices = []
    
    try:
        print(f"[{thread_id}] í˜ì´ì§€ {page_num}/{pages_to_scrape} ì‹œì‘...")
        notices = get_notice_list(driver, page_num)
        
        for notice in notices:
            try:
                detail_data = scrape_notice_detail(driver, notice['ntt_sn'])
                # ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì™€ ë³‘í•©
                detail_data['number'] = notice['number']
                if 'title' not in detail_data:
                    detail_data['title'] = notice['title']
                if 'author' not in detail_data:
                    detail_data['author'] = notice['author']
                if 'date' not in detail_data:
                    detail_data['date'] = notice['date']
                if 'views' not in detail_data:
                    detail_data['views'] = notice['views']
                
                page_notices.append(detail_data)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                with progress_lock:
                    progress_data['completed'] += 1
                    if progress_data['completed'] % 10 == 0:
                        print(f"ì§„í–‰ ìƒí™©: {progress_data['completed']}/{progress_data['total']} ê²Œì‹œê¸€ ì™„ë£Œ")
                
                time.sleep(0.05)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ (ë³‘ë ¬ì´ë¯€ë¡œ ë” ì§§ê²Œ)
                
            except Exception as e:
                print(f"[{thread_id}] ê²Œì‹œê¸€ {notice['number']} í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
                continue
        
        print(f"[{thread_id}] í˜ì´ì§€ {page_num} ì™„ë£Œ ({len(page_notices)}ê°œ ê²Œì‹œê¸€)")
        return page_notices
        
    except Exception as e:
        print(f"[{thread_id}] í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        return []
        
    finally:
        driver.quit()

def save_to_csv(notices, filename='output.csv', detailed_filename='output_detailed.csv'):
    """ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (ê¸°ì¡´ í˜•ì‹ + ìƒì„¸ ì •ë³´)"""
    if not notices:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 1. ê¸°ì¡´ í˜•ì‹ CSV (RAG ì‹œìŠ¤í…œìš©)
    csv_data = []
    for notice in notices:
        # content í•„ë“œ ì •ë¦¬ (ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜)
        content = notice.get('content', '')
        if content:
            content = ' '.join(content.split())  # ì—°ì†ëœ ê³µë°± ì œê±°
        
        row = {
            'title': notice.get('title', ''),
            'content': content,
            'reg_date': notice.get('date', ''),
            'view_count': notice.get('views', ''),
            'notice': notice.get('number', '')
        }
        csv_data.append(row)
    
    # ê¸°ì¡´ í˜•ì‹ CSV ì €ì¥
    df = pd.DataFrame(csv_data)
    df = df[['title', 'content', 'reg_date', 'view_count', 'notice']]
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"CSV íŒŒì¼ ì €ì¥ (ê¸°ì¡´ í˜•ì‹): {filename}")
    
    # 2. ìƒì„¸ ì •ë³´ í¬í•¨ CSV (JSONê³¼ ë™ì¼í•œ ë‚´ìš©)
    detailed_data = []
    for notice in notices:
        # ì²¨ë¶€íŒŒì¼ ì •ë³´ ì •ë¦¬
        attachments = notice.get('attachments', [])
        attachment_names = [att['name'] for att in attachments]
        attachment_count = len(attachments)
        
        # ì´ë¯¸ì§€ ì •ë³´ ì •ë¦¬
        images = notice.get('images', [])
        image_urls = [img['src'] for img in images]
        image_count = len(images)
        
        # ë§í¬ ì •ë³´ ì •ë¦¬
        links = notice.get('links', [])
        link_info = [f"{link['text']} ({link['href']})" for link in links]
        link_count = len(links)
        
        detailed_row = {
            'number': notice.get('number', ''),
            'title': notice.get('title', ''),
            'author': notice.get('author', ''),
            'date': notice.get('date', ''),
            'views': notice.get('views', ''),
            'content': notice.get('content', ''),  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹
            'content_type': notice.get('content_type', ''),
            'url': notice.get('url', ''),
            'ntt_sn': notice.get('ntt_sn', ''),
            'attachment_count': attachment_count,
            'attachment_names': ' | '.join(attachment_names) if attachment_names else '',
            'image_count': image_count,
            'image_urls': ' | '.join(image_urls) if image_urls else '',
            'link_count': link_count,
            'links': ' | '.join(link_info) if link_info else ''
        }
        detailed_data.append(detailed_row)
    
    # ìƒì„¸ ì •ë³´ CSV ì €ì¥
    df_detailed = pd.DataFrame(detailed_data)
    df_detailed.to_csv(detailed_filename, index=False, encoding='utf-8-sig')
    print(f"CSV íŒŒì¼ ì €ì¥ (ìƒì„¸ ì •ë³´): {detailed_filename}")

def get_total_pages(driver):
    """ì „ì²´ í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°"""
    list_url = 'https://www.changwon.ac.kr/ce/na/ntt/selectNttList.do?mi=6627&bbsId=2187'
    driver.get(list_url)
    time.sleep(3)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # goPaging í•¨ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  ë§í¬ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    all_links = soup.find_all('a', onclick=re.compile('goPaging'))
    max_page = 1
    
    for link in all_links:
        onclick = link.get('onclick', '')
        match = re.search(r'goPaging\((\d+)\)', onclick)
        if match:
            page_num = int(match.group(1))
            if page_num > max_page:
                max_page = page_num
    
    # ì „ì²´ ê²Œì‹œê¸€ ìˆ˜ì—ì„œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë„ ì‹œë„
    page_text = soup.get_text()
    total_pattern = re.search(r'ì´\s*([0-9,]+)\s*ê±´', page_text)
    if total_pattern:
        total_posts = int(total_pattern.group(1).replace(',', ''))
        calculated_pages = (total_posts + 14) // 15  # í˜ì´ì§€ë‹¹ 15ê°œ
        max_page = max(max_page, calculated_pages)
    
    print(f"ê°ì§€ëœ ì „ì²´ í˜ì´ì§€ ìˆ˜: {max_page}")
    return max_page

def main(pages=1, max_workers=5):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬)"""
    # ì´ˆê¸° ë“œë¼ì´ë²„ë¡œ ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
    initial_driver = setup_driver()
    
    try:
        # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        total_pages = get_total_pages(initial_driver)
        print(f"ì „ì²´ í˜ì´ì§€ ìˆ˜: {total_pages}")
        
        # í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ ê²°ì •
        pages_to_scrape = min(pages, total_pages)
        print(f"\n{pages_to_scrape}í˜ì´ì§€ë¥¼ {max_workers}ê°œì˜ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ í¬ë¡¤ë§í•©ë‹ˆë‹¤...")
        
        # ì˜ˆìƒ ê²Œì‹œê¸€ ìˆ˜ ì„¤ì • (ì§„í–‰ë¥  í‘œì‹œìš©)
        progress_data['total'] = pages_to_scrape * 15
        
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        all_notices = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ê° í˜ì´ì§€ì— ëŒ€í•œ ì‘ì—… ì œì¶œ
            future_to_page = {
                executor.submit(scrape_page, page, pages_to_scrape): page 
                for page in range(1, pages_to_scrape + 1)
            }
            
            # ì™„ë£Œëœ ì‘ì—…ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_page):
                page_num = future_to_page[future]
                try:
                    page_notices = future.result()
                    all_notices.extend(page_notices)
                except Exception as e:
                    print(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        all_notices.sort(key=lambda x: int(x.get('number', '0')), reverse=True)
        
        # ê²°ê³¼ ì €ì¥ (JSONê³¼ CSV ëª¨ë‘)
        json_file = 'cwnu_all_notices_parallel.json'
        csv_file = 'output.csv'
        
        # JSON ì €ì¥
        with file_lock:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(all_notices, f, ensure_ascii=False, indent=2)
        
        # CSV ì €ì¥
        save_to_csv(all_notices, csv_file)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print(f"\nâœ… ë³‘ë ¬ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ì´ {len(all_notices)}ê°œì˜ ê²Œì‹œê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        print(f"ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
        print(f"JSON íŒŒì¼ ì €ì¥: {json_file}")
        
        # í†µê³„ ì¶œë ¥
        print("\nğŸ“Š ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ í†µê³„:")
        content_types = {}
        for notice in all_notices:
            ct = notice.get('content_type', 'unknown')
            content_types[ct] = content_types.get(ct, 0) + 1
        
        for ct, count in content_types.items():
            print(f"  - {ct}: {count}ê°œ")
        
        # ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ê²Œì‹œê¸€ ìˆ˜
        with_attachments = sum(1 for n in all_notices if n.get('attachments'))
        print(f"  - ì²¨ë¶€íŒŒì¼ í¬í•¨: {with_attachments}ê°œ")
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        
    finally:
        initial_driver.quit()

if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        if arg.lower() == 'all':
            pages = 9999  # ì¶©ë¶„íˆ í° ìˆ˜
        else:
            try:
                pages = int(arg)
            except:
                pages = 1
    else:
        pages = 1
    
    # ìŠ¤ë ˆë“œ ìˆ˜ (ì„ íƒì )
    max_workers = 5  # ê¸°ë³¸ê°’
    if len(sys.argv) > 2:
        try:
            max_workers = int(sys.argv[2])
            max_workers = min(max_workers, 10)  # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
        except:
            pass
    
    main(pages, max_workers)